{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import optimizers\n",
    "import operators\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "import utils\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying Uncertainty in a Reconstruction\n",
    "\n",
    "Assume we have a great, accurate inversion algorithm and with that, we have computed some solution $x^{\\ast}$ from the data observation $y$ in the inverse problem \n",
    "$$y = A(x) + \\varepsilon$$\n",
    "> How certain are we that $x^{\\ast}$ is a representative solution? \n",
    "> How large are the errors that we could have made because of the stochastic nature of the noise $\\varepsilon$? \n",
    "\n",
    "For instance, consider the following example of making two independent CT scans of the same object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "rng = default_rng(2452)\n",
    "\n",
    "fig = plt.figure(tight_layout=True,figsize=(16,8))\n",
    "gs = gridspec.GridSpec(2, 5)\n",
    "\n",
    "dim=100\n",
    "phantom = utils.get_phantom(dim)\n",
    "theta = np.linspace(0,180, endpoint = False, num=128)\n",
    "R = operators.Radon(theta=theta)\n",
    "sinogram = R(phantom)\n",
    "\n",
    "# two different instantiations of the randomly distributed noise\n",
    "sigma = 0.01\n",
    "noise1 = sigma*rng.normal(size=sinogram.shape)\n",
    "noise2 = sigma*rng.normal(size=sinogram.shape)\n",
    "y1 = sinogram + noise1\n",
    "y2 = sinogram + noise2\n",
    "\n",
    "# compute the minimizer\n",
    "mu_tv = 0.005\n",
    "opt1 = optimizers.split_Bregman_TV(R,y1,np.zeros(phantom.shape),1,mu_tv,max_it=25,verbosity=0,energy_fun=lambda x:np.sqrt(np.sum((R(x)-y1)**2))/dim**2)\n",
    "opt2 = optimizers.split_Bregman_TV(R,y2,np.zeros(phantom.shape),1,mu_tv,max_it=25,verbosity=0,energy_fun=lambda x:np.sqrt(np.sum((R(x)-y2)**2))/dim**2)\n",
    "rec1 = opt1.solve()\n",
    "rec2 = opt2.solve()\n",
    "\n",
    "# plot results\n",
    "ax = [fig.add_subplot(gs[:, 0:2])]\n",
    "for i in range(6): ax.append(fig.add_subplot(gs[i%2,2+i//2]))\n",
    "ax[0].imshow(phantom, cmap='gray',vmin=0,vmax=1), ax[0].set_title('Ground truth')\n",
    "ax[1].imshow(y1, cmap='gray'), ax[1].set_title('Noisy scan 1')\n",
    "ax[2].imshow(y2, cmap='gray'), ax[2].set_title('Noisy scan 2')\n",
    "ax[3].imshow(rec1, cmap='gray'), ax[3].set_title('Reconstruction from scan 1')\n",
    "ax[4].imshow(rec2, cmap='gray'), ax[4].set_title('Reconstruction from scan 2')\n",
    "ax[5].imshow(rec1[dim//2-10:dim//2+10,dim//2-10:dim//2+10], cmap='gray', vmin=0, vmax=0.5), ax[5].set_title('Detail rec. 1 (Contrast enhanced)')\n",
    "ax[6].imshow(rec2[dim//2-10:dim//2+10,dim//2-10:dim//2+10], cmap='gray', vmin=0, vmax=0.5), ax[6].set_title('Detail rec. 2 (Contrast enhanced)')\n",
    "\n",
    "for axes in ax: axes.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer, we have to give some probabilistic meaning to the distribution of possible solutions $x$, noise $\\varepsilon$ and the observed data $y$. We assume that both $x$ and $\\varepsilon$ are instantiations of random variables with certain probability distributions. For the marginal distribution of possible solutions, we write $x \\sim \\pi_0$. The probability of observing $y = A(x)+\\varepsilon$ given some $x$ is denoted $\\pi_{y|x}$.\n",
    "For our purposes we assume that those random variables have an absolutely continuous univariate distribution. The can be \"discribed\" by a probability desnity function in the following way:\n",
    "\n",
    "**1-d probability density function:**\n",
    "$$\n",
    "P[a\\leq X \\leq b]=\\int_a^b f_X(x)dx \\quad (1)\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}[X]=\\int_{\\mathbb{R}} x f_X(x)dx \\quad (2)\n",
    "$$\n",
    "$$\n",
    "Var[X]=\\int_{\\mathbb{R}} (\\mathbb{E}[X]-x)^2 f_X(x)dx \\quad (3)\n",
    "$$\n",
    "\n",
    "### &#128221; <span style=\"color:darkorange\"> Question </span>\n",
    "What properties should a probability density function have?\\\n",
    "How dose a multidimensional probability density function look like compared to (1-3) ?\n",
    "\n",
    "\n",
    "$\\pi_0$ is called the **prior distribution**, it encodes prior information, assumptions or belief about the possible nature of our reconstruction $x$. Assume that $\\pi_0$ has a probability density function (pdf) $p_0(x)$.\n",
    "\n",
    "The density $p(\\cdot|x)$ of the distribution $\\pi_{y|x}$ is called the **likelihood**.\n",
    "\n",
    "Once we make an observation $y$, by **Bayes' law** the **posterior distribution** of $x$ after seeing $y$ has the density\n",
    "$$ p(x|y) = \\frac{p(y|x)p_0(x)}{p(y)} \\propto p(y|x) p_0(x). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior distribution\n",
    "Examples\n",
    "- If we believe our solution $x$ should be close to some a priori known value $\\hat x$, then our prior might look something like $p_0(x) = \\exp(-\\frac{1}{2\\sigma^2}||x-\\hat x||_2^2)/Z$. The normalization constant $Z$ ensures that $p_0$ is a valid pdf, we omit it from now on and write \"$\\propto$\" for proportionality. According to this prior density, values closer to $\\hat x$ are assigned larger probability values, since they are assumed to be more likely.\n",
    "- If we believe that $x$ is likely to have little edges/contrast jumps, we might pick a prior like $p_0(x) \\propto \\exp(-\\mathrm {TV}(x)) = \\exp(- \\lVert \\nabla x \\rVert_1)$.\n",
    "- In general, distributions of the form $ p_0(x) \\propto \\exp(-G(x))$ for any $G$ are called **Gibbs priors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise distribution & likelihood function\n",
    "- Homoschedastic Gaussian observation noise $\\varepsilon \\sim \\mathrm{N}(0,I)$ corresponds to a likelihood of the form $ p(y|x) \\propto \\exp(- ||y-A(x)||_2^2 ) $.\n",
    "- If the observed variable $y$ follows a Poisson distribution $y \\sim \\mathrm{Pois}(A(x))$, then $p(y|x) \\propto \\frac 1{y!} \\exp(-A(x)) A(x)^y $ for all $y \\in \\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Question </span>\n",
    "What is the relation of the Maximum likelihood Estimatior\n",
    "$$ \\arg\\max_x p(x|y)  $$\n",
    "for $p_0(x)\\propto\\exp(- \\lambda\\mathrm {TV}(x))$ and $p(y|x)\\propto\\exp(- ||y-A(x)||_2^2 )$ and $\\lambda>0$ to classical variational inversion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Example\n",
    "As a simple example, consider $x\\in \\mathbb{R}$ just to be a number that we want to estimate from the inverse problem $y = a \\cdot x + \\varepsilon$ with $a = 4$. We make one observation of the random variable $y$.\n",
    "\n",
    "We assume that the noise is distributed like $\\varepsilon \\sim \\mathrm N(0,\\sigma^2)$ and a prior $x \\sim \\mathrm N(\\mu_{0}, 1)$.\\\n",
    "Play with the sliders below to get some intuition on how the posterior changes depending on the observation and the parameters in prior and likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529b8d3c544d43c0a9b76ccb5aca80de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='sigma', max=4.0, min=0.1), FloatSlider(value=0.0, deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_1dposterior(sigma, mu0, y):\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "    # prior\n",
    "    t = np.linspace(-5,5,500)\n",
    "    ax[0].plot(t,1/np.sqrt(2*np.pi)*np.exp(-(t-mu0)**2/2))\n",
    "    ax[0].set_title('Prior distribution p(x)')\n",
    "\n",
    "    # likelihood\n",
    "    a = 4               # the \"forward model\"\n",
    "    ax[1].plot(t,1/(np.sqrt(2*np.pi)*sigma)*np.exp(-(y-a*t)**2/(2*sigma**2)))\n",
    "    ax[1].set_title('Likelihood function p(y|x)')\n",
    "\n",
    "    # posterior distribution\n",
    "    ax[2].plot(t,np.exp(-(t-mu0)**2/2 -(y-a*t)**2/(2*sigma**2)))\n",
    "    ax[2].scatter(1/(1+a**2/sigma**2)*(mu0 + a/sigma**2 * y),0,c='r',marker='x')\n",
    "    ax[2].set_yticks([])\n",
    "    ax[2].set_title('Posterior distribution p(x|y)')\n",
    "\n",
    "interactive_plot = interactive(plot_1dposterior, sigma = (0.1,4.0,0.1), mu0 = (-2.0,2.0,0.1), y = (-10.0,10.0,0.1))\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the posterior\n",
    "Let's consider a simple **image denoising** example where we measure a noisy image from\n",
    "$$ y = x + \\varepsilon $$\n",
    "where $x$ is some image. Suppose the errors $\\varepsilon$ are again distributed as mean-zero Gaussian with standard deviation $\\sigma$.\n",
    "\n",
    "In order to regularize, we assume $x$ is sparse in some basis $D$ (you've seen the example with $D$ a Wavelet transform in Tutorial 2).\\\n",
    "The corresponding Gibbs prior has density\n",
    "$$ p_0(x) \\propto \\exp(-|| Dx ||_1). $$\n",
    "\n",
    "We want to compute some statistics of the posterior distribution:\n",
    "- The posterior mean $\\mu_{x|y} := \\mathbb E_{x|y}[x]$\n",
    "- The posterior (pixel-wise) variance $ \\sigma^2_{x|y} := \\mathbb E_{x|y}[(x- \\mu_{x|y})^2] $\n",
    "\n",
    "In order to compute $\\mu_{x|y}$ and $\\sigma^2_{x|y}$, we need to draw samples from the posterior $\\pi_{x|y}$. We use a Markov chain Monte Carlo algorithm based on Langevin diffusion, which iterates\n",
    "$$ \\begin{array}{l} \\xi^{k+1} \\sim \\mathrm N (0, I), \\\\\n",
    "X_{k+1} = X_k - \\tau \\nabla V(X_k) + \\sqrt{2\\tau} \\xi^{k+1}.\n",
    "\\end{array} $$\n",
    "Here $ V $ is the negative log-density of the posterior: $V(x) = -\\log p_{x|y}(x)$.\n",
    "\n",
    "### &#128221; <span style=\"color:darkorange\"> Tasks </span>\n",
    "The following code computes the minimizer $\\hat x = \\min V(x)$, initializes $X_0 = \\hat x$ and then runs the above Langevin sampling algorithm.\n",
    "- Compute an estimator for the posterior mean and plot it in ax[0,2].\n",
    "- Extract two arbitrary samples from Markov chain and plot them in ax[1,0] and ax[1,1]. How do the samples differ qualitatively from $\\hat x$ and the posterior mean?\n",
    "- Can you come up with an efficient way to compute an estimator of $\\sigma^2_{x|y}$? Plot $\\sigma^2_{x|y}$ for each pixel in ax[1,2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m phantom \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_phantom(dim)\n\u001b[0;32m      8\u001b[0m wl \u001b[38;5;241m=\u001b[39m pywt\u001b[38;5;241m.\u001b[39mWavelet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m y \u001b[38;5;241m=\u001b[39m phantom \u001b[38;5;241m+\u001b[39m sigma\u001b[38;5;241m*\u001b[39m\u001b[43mrng\u001b[49m\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39m(dim,dim))\n\u001b[0;32m     10\u001b[0m opti \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mista_wavelets(operators\u001b[38;5;241m.\u001b[39mIdentity(), np\u001b[38;5;241m.\u001b[39mzeros_like(phantom), y, wave \u001b[38;5;241m=\u001b[39m wl, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lamda\u001b[38;5;241m=\u001b[39mlamda\u001b[38;5;241m*\u001b[39msigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,  verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_it\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m opti\u001b[38;5;241m.\u001b[39msolve()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rng' is not defined"
     ]
    }
   ],
   "source": [
    "sigma = 0.1\n",
    "lamda = 30\n",
    "t = sigma**2\n",
    "\n",
    "dim=256\n",
    "phantom = utils.get_phantom(dim)\n",
    "\n",
    "wl = pywt.Wavelet('haar')\n",
    "y = phantom + sigma*rng.normal(size=(dim,dim))\n",
    "opti = optimizers.ista_wavelets(operators.Identity(), np.zeros_like(phantom), y, wave = wl, t=1, lamda=lamda*sigma**2,  verbosity=0, max_it=1)\n",
    "opti.solve()\n",
    "x = np.copy(opti.x)\n",
    "\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,10))\n",
    "# Look here to see how to plot !!!\n",
    "ax[0,0].imshow(y, cmap='gray', vmin=0, vmax = 1)\n",
    "ax[0,0].set_title('Noisy Observation')\n",
    "ax[0,1].imshow(x, cmap='gray', vmin=0, vmax = 1)\n",
    "ax[0,1].set_title('Denoised minimizer $\\hat x$')\n",
    "\n",
    "n_samples = 1000\n",
    "log_post_dens = np.zeros((n_samples,))\n",
    "\n",
    "running_sum = np.zeros_like(x)\n",
    "running_square_sum = np.zeros_like(x)\n",
    "x_hist = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Langevin sampling step\n",
    "    z = x - t /sigma**2 * (x - y) + np.sqrt(2*t)*rng.normal(size=(dim,dim))\n",
    "    wv_coeffs, slices = pywt.coeffs_to_array(pywt.wavedec2(z, wavelet=wl, mode='periodization'))\n",
    "    wv_coeffs_shr = operators.soft_shrinkage(wv_coeffs, lamda * t)\n",
    "    # For each iteration x now contains a sample of the posterior distribution generated by the Langevin sampling algorithem\n",
    "    x = pywt.waverec2(pywt.array_to_coeffs(wv_coeffs_shr, slices, output_format='wavedec2'), wavelet=wl, mode='periodization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
